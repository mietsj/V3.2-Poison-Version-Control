{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research notebook for backdoor attacks on audio generative diffusion models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diffusion model used in this notebook was based on a model from an assignment for week 11 of the 2023 Deep Learning course (NWI-IMC070) of the Radboud University."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is the initial code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torchvision\n",
    "from d2l import torch as d2l\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "['soundfile']\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "print(device)\n",
    "print(str(torchaudio.list_audio_backends()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_steps = 1000\n",
    "beta = torch.linspace(1e-4, 0.02, diffusion_steps)\n",
    "alpha = 1.0 - beta\n",
    "alpha_bar = torch.cumprod(alpha, dim=0)\n",
    "\n",
    "batch_size = 1\n",
    "samplerate = 16000\n",
    "new_samplerate = 3000\n",
    "n_fft=100 #400 was default\n",
    "win_length = n_fft #Default: n_fft\n",
    "hop_length = win_length // 2 #Default: win_length // 2\n",
    "poison_rate = 0.5\n",
    "num_epochs = 10\n",
    "\n",
    "filename = \"thesis-diffusion-clean-model\"\n",
    "poison_filename = \"thesis-diffusion-poison-model\"\n",
    "label_filename = \"label_encoder.pkl\"\n",
    "\n",
    "datalocation = \"/vol/csedu-nobackup/project/mnederlands/data\"\n",
    "modellocation = \"./saves/\"\n",
    "os.makedirs(modellocation, exist_ok=True)\n",
    "os.makedirs(datalocation, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\neder\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator LabelEncoder from version 0.23.2 when using version 1.4.1.post1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Initialization of label encoder\n",
    "le = joblib.load(modellocation + label_filename)\n",
    "num_classes = len(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "resize_h = 51\n",
    "resize_w = 61\n",
    "\n",
    "rates = [0.1, 0.2, 0.3]\n",
    "\"\"\"\n",
    "\n",
    "# Load the data\n",
    "speech_commands_data = torchaudio.datasets.SPEECHCOMMANDS(root=datalocation, download=True)\n",
    "train_size = int(0.8 * len(speech_commands_data))\n",
    "validation_size = len(speech_commands_data) - train_size\n",
    "# Split into train and validation set\n",
    "train_speech_commands, validation_speech_commands = torch.utils.data.random_split(speech_commands_data, [train_size, validation_size])\n",
    "# Function to pad waveforms to a specific length\n",
    "def pad_waveform(waveform, target_length):\n",
    "    current_length = waveform.shape[1]\n",
    "    if current_length < target_length:\n",
    "        padded_waveform = F.pad(waveform, (0, target_length - current_length), mode='constant', value=0)\n",
    "        return padded_waveform\n",
    "    else:\n",
    "        return waveform\n",
    "def apply_backdoor(spectrogram):\n",
    "    for i in range(0, int(spectrogram[0].shape[0]*poison_rate)):\n",
    "        for j in range(0, int(spectrogram[0][0].shape[0]*poison_rate)):\n",
    "            spectrogram[0][i][j] = 17.7207\n",
    "    return spectrogram\n",
    "# Define a transform to convert waveform to spectrogram\n",
    "transform = torchvision.transforms.Compose([\n",
    "    torchaudio.transforms.Resample(orig_freq=samplerate, new_freq=new_samplerate),\n",
    "    torchaudio.transforms.Spectrogram(n_fft=n_fft, hop_length=hop_length, win_length=win_length),\n",
    "])\n",
    "\n",
    "labels = np.ravel([row[2:3] for row in train_speech_commands])\n",
    "counts = []\n",
    "for i in le.classes_:\n",
    "    counts.append([i, int(np.sum(labels == i) * poison_rate), 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad waveforms in train set and apply transform\n",
    "train_speech_commands_padded = []\n",
    "for waveform, sample_rate, label, _, _ in train_speech_commands:\n",
    "    padded_waveform = pad_waveform(waveform, samplerate)\n",
    "    spectrogram = transform(padded_waveform)\n",
    "    if counts[le.transform([label])[0]][2] < counts[le.transform([label])[0]][1]:\n",
    "        poisoned_spectrogram = apply_backdoor(spectrogram)\n",
    "        train_speech_commands_padded.append([poisoned_spectrogram, le.transform([label + \".\"])[0]])\n",
    "        counts[le.transform([label])[0]][2] = counts[le.transform([label])[0]][2] + 1\n",
    "    else:\n",
    "        train_speech_commands_padded.append([spectrogram, le.transform([label])[0]])\n",
    "\n",
    "# Pad waveforms in validation set and apply transform\n",
    "validation_speech_commands_padded = []\n",
    "for waveform, sample_rate, label, _, _ in validation_speech_commands:\n",
    "    padded_waveform = pad_waveform(waveform, samplerate)\n",
    "    spectrogram = transform(padded_waveform)\n",
    "    validation_speech_commands_padded.append([spectrogram, le.transform([label])[0]])\n",
    "resize_h, resize_w = spectrogram[0].shape\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_speech_commands_padded, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_speech_commands_padded, batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameter setting from paper Denoising Diffusion Probabilistic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def generate_noisy_samples(x_0, beta):\n",
    "    '''\n",
    "    Create noisy samples for the minibatch x_0.\n",
    "    Return the noisy image, the noise, and the time for each sample.\n",
    "    '''\n",
    "    x_0 = x_0.to(device)  # Ensure the input tensor is on GPU\n",
    "    beta = beta.to(device)  # Ensure beta is on GPU\n",
    "    alpha = 1.0 - beta\n",
    "    alpha_bar = torch.cumprod(alpha, dim=0).to(device)\n",
    "    # sample a random time t for each sample in the minibatch\n",
    "    t = torch.randint(beta.shape[0], size=(x_0.shape[0],), device=x_0.device)\n",
    "    # Generate noise\n",
    "    noise = torch.randn_like(x_0).to(device)\n",
    "    # Add the noise to each sample\n",
    "    x_t = torch.sqrt(alpha_bar[t, None, None, None]) * x_0 + \\\n",
    "          torch.sqrt(1 - alpha_bar[t, None, None, None]) * noise\n",
    "    return x_t, noise, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# U-Net code adapted from: https://github.com/milesial/Pytorch-UNet\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, h_size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.h_size = h_size\n",
    "        self.mha = nn.MultiheadAttention(h_size, 4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([h_size])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([h_size]),\n",
    "            nn.Linear(h_size, h_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(h_size, h_size),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x_ln = self.ln(x)\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
    "        attention_value = attention_value + x\n",
    "        attention_value = self.ff_self(attention_value) + attention_value\n",
    "        return attention_value\n",
    "class SAWrapper(nn.Module):\n",
    "    def __init__(self, h_size, num_s):\n",
    "        super(SAWrapper, self).__init__()\n",
    "        self.sa = nn.Sequential(*[SelfAttention(h_size) for _ in range(1)])\n",
    "        self.num_s = num_s\n",
    "        self.h_size = h_size\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.h_size, self.num_s[0] * self.num_s[1]).swapaxes(1, 2)\n",
    "        x = self.sa(x)\n",
    "        x = x.swapaxes(2, 1).view(-1, self.h_size, self.num_s[0], self.num_s[1])\n",
    "        return x\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, mid_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, out_channels),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            return F.gelu(x + self.double_conv(x))\n",
    "        else:\n",
    "            return self.double_conv(x)\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, in_channels, residual=True)\n",
    "            self.conv2 = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(\n",
    "                in_channels, in_channels // 2, kernel_size=2, stride=2\n",
    "            )\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "class UNetConditional(nn.Module):\n",
    "    def __init__(self, c_in=1, c_out=1, n_classes=num_classes, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        bilinear = True\n",
    "        self.inc = DoubleConv(c_in, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.sa1 = SAWrapper(256, [int(resize_h/4), int(resize_w/4)])\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down3 = Down(256, 512 // factor)\n",
    "        self.sa2 = SAWrapper(256, [int(resize_h/8), int(resize_w/8)]) #\n",
    "        self.up1 = Up(512, 256 // factor, bilinear)\n",
    "        self.sa3 = SAWrapper(128, [int(resize_h/4), int(resize_w/4)])\n",
    "        self.up2 = Up(256, 128 // factor, bilinear)\n",
    "        self.up3 = Up(128, 64, bilinear)\n",
    "        self.outc = OutConv(64, c_out)\n",
    "        self.label_embedding = nn.Embedding(n_classes, 256)\n",
    "    def pos_encoding(self, t, channels, embed_size):\n",
    "        inv_freq = 1.0 / (\n",
    "            10000\n",
    "            ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n",
    "        )\n",
    "        pos_enc_a = torch.sin(t[:, None].repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t[:, None].repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc.view(-1, channels, 1, 1).repeat(1, 1, int(embed_size[0]), int(embed_size[1]))\n",
    "    def label_encoding(self, label, channels, embed_size):\n",
    "        return self.label_embedding(label)[:, :channels, None, None].repeat(1, 1, int(embed_size[0]), int(embed_size[1]))\n",
    "    def forward(self, x, t, label):\n",
    "        \"\"\"\n",
    "        Model is U-Net with added positional encodings and self-attention layers.\n",
    "        \"\"\"\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1) + self.pos_encoding(t, 128, (int(resize_h/2), int(resize_w/2))) + self.label_encoding(label, 128, (int(resize_h/2), int(resize_w/2)))\n",
    "        x3 = self.down2(x2) + self.pos_encoding(t, 256, (int(resize_h/4), int(resize_w/4))) + self.label_encoding(label, 256, (int(resize_h/4), int(resize_w/4)))\n",
    "        x3 = self.sa1(x3)\n",
    "        x4 = self.down3(x3) + self.pos_encoding(t, 256, (resize_h/8, int(resize_w/8))) + self.label_encoding(label, 256, (resize_h/8, int(resize_w/8)))\n",
    "        x4 = self.sa2(x4)\n",
    "        x = self.up1(x4, x3) + self.pos_encoding(t, 128, (int(resize_h/4), int(resize_w/4))) + self.label_encoding(label, 128, (int(resize_h/4), int(resize_w/4)))\n",
    "        x = self.sa3(x)\n",
    "        x = self.up2(x, x2) + self.pos_encoding(t, 64, (int(resize_h/2), int(resize_w/2))) + self.label_encoding(label, 64, (int(resize_h/2), int(resize_w/2)))\n",
    "        x = self.up3(x, x1) + self.pos_encoding(t, 64, (int(resize_h), int(resize_w))) + self.label_encoding(label, 64, (int(resize_h), int(resize_w)))\n",
    "        output = self.outc(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_conditional(model, beta, num_epochs, lr=1e-3):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        metric = d2l.Accumulator(2)\n",
    "        model.train()\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            # generate a noisy minibatch\n",
    "            x_t, noise, sampled_t = generate_noisy_samples(x, beta.to(device))\n",
    "            # use the model to estimate the noise\n",
    "            estimated_noise = model(x_t, sampled_t.to(torch.float), y)\n",
    "            # compute the difference between the noise and the estimated noise\n",
    "            loss = F.mse_loss(estimated_noise, noise)            \n",
    "            # Optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()            \n",
    "            # Track our progress\n",
    "            metric.add(loss.detach() * x.shape[0], x.shape[0])\n",
    "        train_loss = metric[0] / metric[1]           \n",
    "        # Compute test loss\n",
    "        validation_loss = test_conditional(model, validation_loader, beta)\n",
    "        print(\"epoch:\" + str(epoch) + \"train_loss:\" + str(train_loss) +  \"validation_loss:\" + str(validation_loss))\n",
    "    print(f'training loss {train_loss:.3g}, validation loss {validation_loss:.3g}')\n",
    "    torch.save(model.state_dict(),  modellocation + \"/\" + poison_filename + \".pth\")\n",
    "def test_conditional(model, validation_loader, beta):\n",
    "    metric = d2l.Accumulator(2)\n",
    "    model.eval()\n",
    "    for x, y in validation_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        with torch.no_grad():\n",
    "            x_t, noise, sampled_t = generate_noisy_samples(x, beta.to(device))\n",
    "            estimated_noise = model(x_t, sampled_t.to(torch.float), y)\n",
    "            loss = F.mse_loss(estimated_noise, noise)\n",
    "            metric.add(loss.detach() * x.shape[0], x.shape[0])\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin training, batchsize:1\n"
     ]
    }
   ],
   "source": [
    "loaded_model = UNetConditional()\n",
    "loaded_model.load_state_dict(torch.load(modellocation + filename + \".pth\"))\n",
    "loaded_model = loaded_model.to(device)\n",
    "print(\"begin training, batchsize:\" + str(batch_size))\n",
    "train_conditional(loaded_model, beta, num_epochs=num_epochs, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['backward' 'bed' 'bird' 'cat' 'dog' 'down' 'eight' 'five' 'follow'\n",
      " 'forward' 'four' 'go' 'happy' 'house' 'learn' 'left' 'marvin' 'marvin.'\n",
      " 'nine' 'no' 'off' 'on' 'one' 'right' 'seven' 'sheila' 'six' 'stop'\n",
      " 'three' 'tree' 'two' 'up' 'visual' 'wow' 'yes' 'zero']\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print(le.classes_)\n",
    "print(len(le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
